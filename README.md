# ğŸ§  ML Data Engineering Habits (but make it âœ¨cleanâœ¨)

![Personal Repo](https://img.shields.io/badge/ğŸ”’-personal_repo-blueviolet?style=for-the-badge)

Welcome to the **data engineer's survival kit** â€” a collection of Python + shell scripts showing how to actually keep your ML data projects from turning into chaos.  
We're talkin' **validation**, **versioning**, **lineage**, and **vibes** (the observability kind).

## ğŸ—‚ï¸ What's Inside

* `log_data_lineage.py` â†’ Keeps receipts (logs your DataFrame metadata for traceability).
* `validate_data_schema.py` â†’ Checks your data before it embarrasses you in prod.
* `version_data_with_dvc.sh` â†’ Handles dataset versioning with DVC like a boss.
* `detect_feature_drift.py` â†’ Calls out data drift before it ruins your model's day.
* `prototype_etl_sampling.py` â†’ Makes sampling huge datasets actually chill for prototyping.
* `register_data_catalog.py` â†’ Gets your datasets living rent-free in a metadata catalog.
* `check_data_freshness.py` â†’ Tells you if your data's getting crusty (freshness checks ğŸ”).

## âš™ï¸ How to Use

Run each script solo using Python (or bash if it's the `.sh` one).  
You'll need **Git** + **DVC** for the versioning magic.

```sh
python validate_data_schema.py
bash version_data_with_dvc.sh
```

## ğŸš€ Setup (fr, it's easy)

Use **uv** to install what you need:

```sh
uv add pandas pandera scipy dvc
# or:
uv sync
```

## ğŸ’¡ TL;DR

These scripts = good data habits.  
Good data habits = happy ML pipeline.  
Happy ML pipeline = less 3AM debugging. ğŸ§˜â€â™‚ï¸
